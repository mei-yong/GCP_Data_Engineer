
Google Cloud Storage

Object storage - anything from CSV to JSON to images to audio to video files.
Store objects in buckets. Buckets are associated with a project. You can group projects under an organisation.
Bucket names must be globally unique - common practice is to name it as 'project_id-bucket_descriptor' since project ID is also globally unique.
Is a Hadoop Compatible File System (HCFS) - but is faster than HDFS
Regional Cloud Storage is cheaper than BQ storage


Storage Classes + Location
https://cloud.google.com/storage/docs/storage-classes
Regional - updated whenever - lower latency than multi-regional if bucket is in same place as compute
Multi-regional - updated whenever
Nearline - expected to be updated once a month
Coldline - expected to be updated once a year


Reference: Uploading objects
https://cloud.google.com/storage/docs/uploading-objects#storage-upload-object-python


--------------------------------------------------------------

Shell scripts

Reference: All key shell scripts
https://cloud.google.com/storage/docs/how-to

# Set bucket name variable using the default project ID detected by Shell
export BUCKET_NAME=$DEVSHELL_PROJECT_ID

# Change default storage class
gsutil defstorageclass set [STORAGE_CLASS] gs://[BUCKET_NAME]

# Make a new bucket
gsutil mb gs://$BUCKET_NAME/
gsutil mb -p [PROJECT_NAME] -c [STORAGE_CLASS] -l [BUCKET_LOCATION] -b on gs://[BUCKET_NAME]/
  
-p: Specify the project with which your bucket will be associated. For example, my-project.
-c: Specify the default storage class of your bucket. For example, NEARLINE.
-l: Specify the location of your bucket. For example, US-EAST1.
-b: Enable uniform bucket-level access for your bucket.

# List all buckets in a project
gsutil ls

# List all objects in a bucket
gsutil ls -r gs://[BUCKET_NAME]/**

# Get bucket size in bytes
# Note: The gsutil du command calculates space usage by making object listing requests, which can take a long time for large buckets. If the number of objects in your bucket is hundreds of thousands or more, use Stackdriver instead.
gsutil du -s gs://$BUCKET_NAME/

# Get bucket metadata
gsutil ls -L -b gs://$BUCKET_NAME/

# Get object metadata
gsutil stat gs://[BUCKET_NAME]/[OBJECT_NAME]

# Edit object metadata
gsutil setmeta -h "[METADATA_KEY]:[METADATA_VALUE]" gs://[BUCKET_NAME]/[OBJECT_NAME]

[METADATA_KEY] is the key for the metadata you want to set. For example, Content-Type.
[METADATA_VALUE] is the value for the metadata you want to set. For example, image/png.
[BUCKET_NAME] is the name of the bucket containing the object whose metadata you want to edit. For example, my-bucket.
[OBJECT_NAME] is the name of the object whose metadata you want to edit. For example, pets/dog.png.

# Upload an object - local -> GCS
gsutil cp [LOCAL_OBJECT_LOCATION] gs://[DESTINATION_BUCKET_NAME]/
e.g. gsutil cp Desktop/data/cat.png gs://my-project-id/data/

# Download an object - GCS -> local
gsutil cp gs://[BUCKET_NAME]/[OBJECT_NAME] [SAVE_TO_LOCATION]
e.g. gsutil cp gs://my-project-id/data/cat.png Desktop/data

# Copy all objects from one bucket to another
# the -r option recursively copies all your objects
# the * represents wildcard
gsutil cp -r gs://[SOURCE_BUCKET]/* gs://[DESTINATION_BUCKET]

# Rename a single object in a bucket
gsutil mv gs://[BUCKET_NAME]/[OLD_OBJECT_NAME] gs://[BUCKET_NAME]/[NEW_OBJECT_NAME]

# Copy a single object from one bucket to another
gsutil cp gs://[SOURCE_BUCKET_NAME]/[SOURCE_OBJECT_NAME] gs://[DESTINATION_BUCKET_NAME]/[NAME_OF_COPY]

# Move (and optionally rename) a single object from one bucket to another
gsutil mv gs://[SOURCE_BUCKET_NAME]/[SOURCE_OBJECT_NAME] gs://[DESTINATION_BUCKET_NAME]/[DESTINATION_OBJECT_NAME]

# Delete all objects in a bucket but not the bucket itself
gsutil rm -a gs://[SOURCE_BUCKET]/**

# Delete the whole bucket and all its objects
gsutil rm -r gs://[SOURCE_BUCKET]

# View the Cloud IAM policy for a bucket
gsutil iam get gs://[BUCKET_NAME]

# Add a member to a bucket-level policy
gsutil iam ch [MEMBER_TYPE]:[MEMBER_NAME]:[IAM_ROLE] gs://[BUCKET_NAME]

# Remove a member from a bucket-level policy
gsutil iam ch -d [MEMBER_TYPE]:[MEMBER_NAME] gs://[BUCKET_NAME]

[MEMBER_TYPE] is the type of member to which you are granting bucket access. For example, user.
[MEMBER_NAME] is the name of the member to which you are granting bucket access. For example, jane@gmail.com.
[IAM_ROLE] is the IAM role you are granting to the member. For example, roles/storage.objectCreator.
[BUCKET_NAME] is the name of the bucket you are granting the member access to. For example, my-bucket.


Reference: IAM
https://cloud.google.com/storage/docs/access-control/using-iam-permissions

Reference: Composite Objects
https://cloud.google.com/storage/docs/composite-objects
https://cloud.google.com/storage/docs/composing-objects

Reference: Resumable Uploads
https://cloud.google.com/storage/docs/performing-resumable-uploads

Reference: Streaming Transfers
https://cloud.google.com/storage/docs/streaming


--------------------------------------------------------------


Using GCS with Python

https://cloud.google.com/storage/docs/reference/libraries#client-libraries-install-python
https://cloud.google.com/storage/docs/downloading-objects#storage-download-object-python
https://cloud.google.com/storage/docs/how-to#working-with-objects


Reference: Using Python to read & write to GCS
https://cloud.google.com/appengine/docs/standard/python/googlecloudstorageclient/read-write-to-cloud-storage

##### In Cloud Shell #####

# Install GCS Python library
pip install --upgrade google-cloud-storage

# Set up a service account
gcloud iam service-accounts create <service_account_name>

# Grant permissions to the service account
gcloud projects add-iam-policy-binding <PROJECT_ID> --member "serviceAccount:<service_account_name>@<PROJECT_ID>.iam.gserviceaccount.com --role "roles/owner"

# Generate the key file
gcloud iam service-accounts keys create <key_file_name>.json --iam-account <service_account_name>@<PROJECT_ID>.iam.gserviceaccount.com


# Provide authentication credentials to your application code by setting the environment variable GOOGLE_APPLICATION_CREDENTIALS. Replace [PATH] with the file path of the JSON file that contains your service account key, and [FILE_NAME] with the filename. This variable only applies to your current shell session, so if you open a new session, set the variable again.
Windows Powershell
$env:GOOGLE_APPLICATION_CREDENTIALS="[PATH]"
Windows Command prompt
set GOOGLE_APPLICATION_CREDENTIALS=[PATH]



##### In Python File #####

# Import library
from google.cloud import storage

# Instantiate a client
storage_client = storage.Client()

# Get file from GCS bucket
def download_blob(bucket_name, source_blob_name, destination_file_name):
    '''Downloads a blob from the bucket'''
    storage_client = storage.Client()
    bucket = storage_client.get_bucket(bucket_name)
    blob = bucket.blob(source_blob_name)

    blob.download_to_filename(destination_file_name)

    print('Blob {} downloaded to {}.'.format(
        source_blob_name,
        destination_file_name))

		
# Upload file to GCS bucket
def upload_blob(bucket_name, source_file_name, destination_blob_name):
    """Uploads a file to the bucket."""
    storage_client = storage.Client()
    bucket = storage_client.get_bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)

    blob.upload_from_filename(source_file_name)

    print('File {} uploaded to {}.'.format(
        source_file_name,
        destination_blob_name))



--------------------------------------------------------------

Signed URLs
Enabling external users and resources to access GCS files

https://cloud.google.com/storage/docs/access-control/signed-urls
https://medium.com/@aejefferson/how-to-use-cloud-storage-to-securely-load-data-into-neo4j-d97b72b2ad8f


# Create service account and return its "email" identifier
service_account_email="$(gcloud iam service-accounts create "url-signer" \
	--display-name "url-signer for items in storage buckets" \
	--format="value(email)")"
	  
# View list of service accounts associated with the project
gcloud iam service-accounts list
service_account_email="url-signer@venti-sandbox-956450.iam.gserviceaccount.com"

# Create a private key for that service account
gcloud iam service-accounts keys create ./url-signer-key.json \
	--iam-account="${service_account_email}"

# Configure the service account we want to use with read permissions on the bucket 
gsutil iam ch serviceAccount:${service_account_email}:objectViewer "gs://<bucket>"

# Generate signed urls
# -m for multithreading, -d for duration of validity, -m for method
gsutil -m signurl -d 6h -m GET \
        "<path>/url-signer-key.json" \
        "gs://<bucket>/<filename.ext>"

URL     HTTP Method     Expiration      Signed URL
gs://meizilla/nodes.csv GET     2020-01-27 20:02:45     https://storage.googleapis.com/meizilla/nodes.csv?x-goog-signature=821852d6a531d8d5292624ac0245a...

# Generated signed urls for all items in a bucket
gsutil -m signurl -d 6h -m GET \
        "<path>/url-signer-key.json" \
        "gs://<bucket>/**"

URL     HTTP Method     Expiration      Signed URL
gs://meizilla/nodes.csv GET     2020-01-27 20:05:14     https://storage.googleapis.com/meizilla/nodes.csv?x-goog-signature=603b28d2281b8b7cf5db61a5c4fe7...
gs://meizilla/rels1.csv GET     2020-01-27 20:05:14     https://storage.googleapis.com/meizilla/rels1.csv?x-goog-signature=2b582ccf269da57bb90a61ebb10ea...
gs://meizilla/rels2.csv GET     2020-01-27 20:05:14     https://storage.googleapis.com/meizilla/rels2.csv?x-goog-signature=75901cf063be6369d3fb3177916428...




