
Google Compute Engine (GCE)


Reference: Connect to Instance
https://cloud.google.com/compute/docs/instances/connecting-to-instance

How to run a Python script in Shell and still have it running even if you exit Shell
https://stackoverflow.com/questions/47455680/running-a-python-script-on-google-cloud-compute-engine

Regions & Zones - namespaces
https://cloud.google.com/compute/docs/regions-zones/
It's good to put bunches of resources in different regions so that if one of them fails, others are still ok
Every resource in Compute Engine must be uniquely named across the project no matter if they're in different regions/zones
The persistent disk and static IP address need to be the in the same zone as the VM instance. The static external IP address can be in a different zone in the same region.


Backing up using snapshots
https://cloud.google.com/compute/docs/disks/restore-and-delete-snapshots
Existing VM disk -> create snapshot -> create VM instance boot disk -> create a new persistent disk when required
Snapshots only hold information about changes to the VM instance (kind of like Git versions) so when you delete old snapshots, information that the subsequent snapshots need to create a boot disk gets transfered to the next latest snapshot


Managed Instances Groups
https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances#monitoring_groups
Good for stateless apps - i.e. apps that don't need specific state of underlying VM instances to run
Identical instances based on an instance template - specify a container image or a custom image
Autohealing, load balancing, autoscaling, auto-updating
Single-zone or multi-zone in a region
Max 1000 VMs
When updating, max 1000 intances can be specified in a single request

------------------------------------------------------

GCE Startup Scripts
https://cloud.google.com/compute/docs/startupscript

### If spinning up a new VM instance

# Script is short enough to include in the gcloud command
gcloud compute instances create example-instance --tags http-server \
	--metadata startup-script='#! /bin/bash
	# Installs apache and a custom homepage
	sudo su -
	apt-get update
	apt-get install -y apache2
	cat <<EOF > /var/www/html/index.html
	<html><body><h1>Hello World</h1>
	<p>This page was created from a simple start up script!</p>
	</body></html>
	EOF'

# Script located inside the VM
gcloud compute instances create example-instance \
    --metadata-from-file startup-script=examples/scripts/install.sh

# Script located inside a GCS bucket
gcloud compute instances create example-instance \
	--scopes storage-ro \
    --metadata startup-script-url=gs://bucket/startupscript.sh

### If a VM instance already exists

# Script located inside the VM
gcloud compute instances add-metadata example-instance \
    --metadata-from-file startup-script=path/to/file

# Script located inside a GCS bucket
gcloud compute instances add-metadata example-instance \
    --metadata startup-script-url=gs://bucket/file

### Rerun a startup script from inside a VM
sudo google_metadata_script_runner --script-type startup --debug


#-----------------------------#
# VM startup-script: template
#-----------------------------#


# Update the OS apps
sudo apt-get update && apt-get upgrade

# Download and install pip
sudo curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
sudo python get-pip.py
sudo pip install -U pip

# Make python 3 the default
sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.5 2
sudo update-alternatives --config python
sudo update-alternatives  --set python /usr/bin/python3.5

# Install Python packages
# google-cloud-storage - for working with GCS bucket
# google-cloud-bigquery - for working with BigQuery
# pyopenssl - for signed URLs
# py2neo - for running Neo4j commits over bolt connection
sudo pip install pandas google-cloud-storage

# Make a directory for all of the subsequent operations
sudo mkdir /home/python-run

########## Optional: for creating signed URLs for GCS bucket objects ##########

# Copy the json url signer from GCS to the dedicated directory
sudo gsutil cp gs://<bucket_name>/<signer_key.json> /home/python-run

# Use the url signer json to generate a signed URL and assign the generated URL to a variable
GENSIGN=$(gsutil -m signurl -d 8h -m GET "<directory_of_signer_key.json>" "gs://<bucket_name>/<file_to_sign.csv>")
SIGN=$(echo "$GENSIGN" | awk 'FNR == 2 {print $5}')

# Delete the json url signer from the dedicated directory
sudo rm /home/python-run/<signer_key.json>

#################################################################################

# Copy the Python file to be executed into the dedicated directory
sudo gsutil cp gs://<bucket_name>/<python_file_name.py> /home/python-run

# Run the Python executable file
sudo python /home/python-run/<python_file_name.py> <optional_python_input1> <optional_python_input2>

# Optional: Shutdown the VM
#sudo shutdown -h now



------------------------------------------------------

Example: Setup NGINX web server on a VM

http://nginx.org/

# See defaults
gcloud compute instances create --help

# Set default zone
gcloud config set compute/zone $ZONE

# Exit from help
Ctrl + C

# Create a new VM instance (don't need zone if already set default)
gcloud compute instances create $INSTANCE_NAME \
--machine-type n1-standard-2 \
--zone $ZONE

# SSH into the VM
gcloud compute ssh $INSTANCE_NAME

# Tunnel into the VM
gcloud compute ssh $INSTANCE_NAME --$ZONE --tunnel-through-iap

# Access root using sudo
sudo su -

# Update OS
apt-get update

# Install NGINX
apt-get install nginx -y

# Check NGINX is running
ps auwx | grep nginx

# Access NGINX by opening the external IP in a web browser window


------------------------------------------------------

Example: 2-Tier Web App with Node.js frontend and MongoDB backend

Create 2 VM instances - one for FE and one for BE
* Give them names
* Choose region & zone
* Machine type micro
* Boot disk Ubuntu 14.04 LTS
* Firewall selector Allow HTTP traffic to open port 80 to access the app

In Cloud Shell

# SSH into the backend VM
gcloud compute --project $PROJECT ssh --zone $BACKEND-ZONE $BACKEND-NAME

# Update packages
sudo apt-get update

# Install MongoDB (it auto runs when you install)
sudo apt-get install mongodb

# Stop the MongoDB
sudo service mongodb stop

# Create a directory for MongoDB
sudo mkdir $HOME/db

# Run the MongoDB service in the background on port 80
sudo mongod --dbpath $HOME/db --port 80 --fork --logpath /var/tmp/mongodb

# Exit & close the backend instance
exit

# SSH into the frontend VM
gcloud compute --project $PROJECT ssh --zone $FRONTEND-ZONE $FRONTEND-NAME

# Update packages
sudo apt-get update

# Install git and npm
curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash -

# Install Node.js
sudo apt-get install git nodejs

# Clone the sample app
git clone https://github.com/GoogleCloudPlatform/todomvc-mongodb.git

# Install app dependencies
cd todomvc-mongodb; npm install
sed -i -e 's/8080/80/g' server.js

# Start the to-do web app
sudo nohup nodejs server.js --be_ip [backend-internal-ip] --fe_ip [frontend-internal-ip] &

# Exit & close the frontend instance
exit

# Access THE APP by opening the external IP in a web browser window


------------------------------------------------------

Example: Network and HTTP Load Balancers
https://cloud.google.com/load-balancing/docs/load-balancing-overview

# Use external load balancing when distributing traffic from the internet to a GCP network

# Create multiple web server instances (a cluster)
# Instance Template - define the specs of every VM in the cluster (i.e. disk, CPUs, memory, etc)
# Managed Instance Groups - instantiate a number of VM instances using the Instance Template

# Create a startup shell script to be used by every VM in the cluster - this example sets up a NGINX server instance
cat << EOF >> startup.sh
### script contents ###
#! /bin/bash
apt-get update
apt-get install -y nginx
service nginx start
sed -i -- 's/nginx/Google Cloud Platform - '"\$HOSTNAME"'/' /var/www/html/index.nginx-debian.html
EOF

# Check the script file was made successfully
ls -la

# Create an instance template using the startup shell script
gcloud compute instance-templates create nginx-template \
--metadata-from-file startup-script=startup.sh

# Create a target pool. Allows a single access point to all the instances in a group and is for load balancing in later steps
# Ensure at this step, the pool is made in the same zone as the cluster
gcloud compute target-pools create nginx-pool

# Create a managed instance group using the instance template
gcloud compute instance-groups managed create nginx-group \
--base-instance-name nginx \
--size 2 \
--template nginx-template \
--target-pool nginx-pool

# View list of compute instances created
gcloud compute instances list

# Configure a firewall so you can connect to the VMs on port 80 via the external IP address
gcloud compute firewall-rules create www-firewall --allow tcp:80

# Can view the NGINX server running in a web browser
http://35.232.106.254/



# L3 Network load balancer
# Unlike HTTP(S) load balancing, can load balance additional TCP/UDP-based protocols (e.g. SMTP traffic) and inspect the packets to see TCP-connection-related characteristics
# It is regional and non-proxied

# Create an L3 network balancer
gcloud compute forwarding-rules create nginx-lb \
--region us-central1 \
--ports=80 \
--target-pool nginx-pool

# View all GCE forwarding rules in the project
gcloud compute forwarding-rules list

NAME      REGION       IP_ADDRESS     IP_PROTOCOL  TARGET
nginx-lb  us-central1  35.193.83.138  TCP          us-central1/targetPools/nginx-pool



# L7 HTTP(S) Load Balancer
https://cloud.google.com/compute/docs/load-balancing/http/

# Global load balancing. Route some URLs to one set of instances and route other URLs to other instances
# Requests are always routed to the instance group that is closest to the user (will pick the next closest if the closest doesn't have enough capacity)

# Create a basic health check to verify that the instance is responding to HTTP(s) traffic
gcloud compute http-health-checks create http-basic-check

# Define an HTTP service and map a port name to the relevant port for the instance group
gcloud compute instance-groups managed \
set-named-ports nginx-group \
--named-ports http:80

# Create a backend service
https://cloud.google.com/compute/docs/reference/rest/v1/backendServices
gcloud compute backend-services create nginx-backend \
--protocol HTTP --http-health-checks http-basic-check --global

# Add the instance group to the backend service
gcloud compute backend-services add-backend nginx-backend \
    --instance-group nginx-group \
    --instance-group-zone us-central1-a \
    --global

# Create a default URL map that directs all incoming requests to all the instances
gcloud compute url-maps create web-map --default-service nginx-backend

# See below link for how to direct traffic to different instances based on the URL being requested (content-based routing)
https://cloud.google.com/load-balancing/docs/https/https-load-balancer-example

# Create a target HTTP proxy to route requests to your URL map
gcloud compute target-http-proxies create http-lb-proxy --url-map web-map

# Create a global forwarding rule to handle and route incoming requests (does not support multiple ports)
gcloud compute forwarding-rules create http-content-rule \
	--global \
	--target-http-proxy http-lb-proxy \
	--ports 80

# Check that the forwarding rule got created
gcloud compute forwarding-rules list

NAME               REGION       IP_ADDRESS     IP_PROTOCOL  TARGET
http-content-rule               35.190.15.116  TCP          http-lb-proxy
nginx-lb           us-central1  35.193.83.138  TCP          us-central1/targetPools/nginx-pool


------------------------------------------------------

Using a VM to execute a Python file - including installing dependencies & signed GCS URLs

# Update the OS apps
sudo apt-get update && apt-get upgrade

# Download and install pip
curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
sudo python get-pip.py
sudo pip install -U pip

# Install Python packages
sudo pip install py2neo pyopenssl

# Make a directory for all of the subsequent operations
sudo mkdir /home/python-run

# Copy the Python file to be executed into the dedicated directory
sudo gsutil cp gs://bucket/test6.py /home/python-run

# Copy the json url signer from GCS to the dedicated directory
sudo gsutil cp gs://project-keys/project-key-123456789.json /home/python-run

# Use the url signer json to generate a signed URL for each of the CSVs and assign the generated URLs to variables
TRANSSIGN=$(gsutil -m signurl -d 8h -m GET "/home/python-run/project-key-123456789.json" "gs://bucket/filename.csv")
TRANSURL=$(echo "$TRANSSIGN" | awk 'FNR == 2 {print $5}')

# Delete the json url signer from the dedicated directory
sudo rm venti-dev-498572-ee1ce9e17158.json

# Run the Python executable file that reads from CSVs in the GCS bucket and builds the Neo4j graph db via the bolt connection
sudo python /home/python-run/test6.py $TRANSURL

# Shutdown the VM
sudo shutdown -h now


-----------------------------
test6.py
-----------------------------
import sys

def create_skeleton(transactions=sys.argv[1]):

    # Import Python libraries
    from py2neo import Graph
    
    # Initialise the graph db
    uri = "bolt://uri:7687"
    user = "neo4j"
    password = "KPMG4j@@"
    graph = Graph(uri=uri, user=user, password=password)
    
    graph.run("CREATE CONSTRAINT ON (b:Business) ASSERT b.id IS UNIQUE")
    
    query = '''
            USING PERIODIC COMMIT 500
            LOAD CSV WITH HEADERS
            FROM "{}" AS row
            WITH row WHERE row.id_1 IS NOT NULL
                AND row.type_id_1 = "Business"
            MERGE (:Businesstest {{id:row.id_1}} )
    '''.format(transactions)
    
    graph.run(query)
    
    query = '''
            USING PERIODIC COMMIT 500
            LOAD CSV WITH HEADERS
            FROM "{}" AS row
            WITH row WHERE row.id_2 IS NOT NULL
                AND row.type_id_2 = "Business"
            MERGE (:Businesstest {{id:row.id_2}} )
    '''.format(transactions)
    
    graph.run(query)
    
    query = '''
            USING PERIODIC COMMIT 500
            LOAD CSV WITH HEADERS
            FROM "{}" AS row
            WITH row WHERE row.id_1 IS NOT NULL
                AND row.id_2 IS NOT NULL
                AND row.transaction_count IS NOT NULL
            MATCH (a:Businesstest {{id:row.id_1}})
            MATCH (b:Businesstest {{id:row.id_2}})
            MERGE (a)-[r:PAID {{network_id: row.network_id,
                                max_date: row.max_date,
                                min_date: row.min_date,
                                transaction_count: toInteger(row.transaction_count),
                                total_value_of_transactions: toFloat(row.total_value_of_transactions)	
                                }}
                        ]->(b) 
    '''.format(transactions)
    
    graph.run(query)

if __name__ == "__main__":
    create_skeleton()



