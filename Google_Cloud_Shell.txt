
References
https://cloud.google.com/sdk/gcloud/
https://cloud.google.com/compute/docs/instances/connecting-to-instance

# Cloud Shell is a Debian-based VM with dev tools like gcloud, git, and others and offers a persistent 5GB home directory.

# See default VM values
gcloud compute instances create â€“help

# List the project ID
gcloud config list project

# Set default region or zone
gcloud config set compute/zone $ZONE
gcloud config set compute/region $REGION

# Check current credentials
gcloud auth list

# Check what permissions you have
gcloud projects get-iam-policy $PROJECT_ID

# Access root using sudo
sudo su -

# Update OS
apt-get update

# Set environment variables
export PROJECT_ID=project_id
export PROJECT_ID=$DEVSHELL_PROJECT_ID

# Check variables
echo $PROJECT_ID

# gcloud help
gcloud -h

# Get more detailed help
gcloud config --help OR gcloud help config
# Exit detailed help
q

# View list of configurations in your environment
gcloud config list

# Check how other properties are set
gcloud config list --all

# View list of all components
gcloud components list

# Change current working directory
cd $HOME

# Open vi editor (like nano)
vi <filepath>

# Close vi editor
ESC + :wq


# Setup Python environment

python3 --version

pip3 --version
https://pip.pypa.io/en/stable/installing/
curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
sudo python get-pip.py

sudo pip3 install -U pip
sudo pip3 install --upgrade virtualenv
virtualenv -p python3.7 my_env
source my_env/bin/activate

------------------------------------------------------------------

Dataflow Example

# Activate the virtual environment
source my_env/bin/activate

# Install Apache Beam
pip install apache-beam[gcp]

# Run the Python file - by default, will use DirectRunner (could specify DataflowRunner in the Python file
python -m apache_beam.examples.wordcount --output OUTPUT_FILE

# Check the output file
ls
cat $FILE

# Run the pipeline remotely

BUCKET=gs://$BUCKET_NAME

python -m apache_beam.examples.wordcount \
	--project $DEVSHELL_PROJECT_ID \
	--runner DataflowRunner \
	--staging_location $BUCKET/staging \
	--temp_location $BUCKET/temp \
	--output $BUCKET/results/output

# Check outputs in the console by navigating to Dataflow and/or GCS





