
Cloud Functions

Serverless, stateless, execution environment for app code
No custom Python libraries so if you need custom ones, consider Cloud Run instead of GKE
Triggered by an event – could be GCS, Pub/Sub, HTTP web events, etc
1)	Authentication
2)	Send watch request – sync notification event
3)	Add, update, remove object
4)	Notification
5)	Waits for acknowledgement
6)	If app unreachable after 20 seconds, notification retired
7)	If app reachable but no acknowledgement, then exponential backoff 30 seconds after fail up to max 90 minutes for up to 7 days

You can specify requirements so that common libraries are loaded into the environment whenever the Function is triggered
Often Functions are used for ETL
Can trigger periodic events using Cloud Scheduler. For data processing, there’s Dataproc Workflow Templates and Cloud Composer
Can monitor functions via Stackdriver
Functions are written in Python, Node.js or Go


For Python
https://cloud.google.com/functions/docs/writing/#functions-writing-file-structuring-python
Main.py – definitions of one or more Cloud Functions
Requirements.txt – used by the Py package manager to incorporate dependencies into runtime environment. For whatever is not available through pip, package these and supply them to the Cloud Functions as well
Cloud Function code can be deployed to the service via Console, gcloud CLI, local computer
The name of the function is defined at deployment and must be unique in the region. By default, the name of the function is also the entry point into the source code – i.e. foo function will execute foo() in the source code – can change using –entry-point flag at deploy time



#------------------------------------------------------------------#
# Function that takes BigQuery tables and outputs them as CSVs
#------------------------------------------------------------------#

from google.cloud import storage
import datetime

def initateProcess(event, context):
    exportToCSV_mldb()
    exportToCSV_risk()
    exportToCSV_transaction()
    archiveData()

def exportToCSV_mldb():
    from google.cloud import storage
    import base64

    from google.cloud import bigquery
    client = bigquery.Client()
    bucket_name = 'venti-dev-498572-bqoutput'
    project = "venti-dev-498572"
    dataset_id = "final_output"
    table_id = "final_mldb"

    destination_uri = "gs://{}/{}".format(bucket_name, "output_mldb.csv")
    dataset_ref = client.dataset(dataset_id, project=project)
    table_ref = dataset_ref.table(table_id)

    extract_job = client.extract_table(
        table_ref,
        destination_uri,
        # Location must match that of the source table.
        location="US",
    )  # API request
    extract_job.result()  # Waits for job to complete.

    print(
        "Exported {}:{}.{} to {}".format(project, dataset_id, table_id, destination_uri)
    )
    
def exportToCSV_risk():
    from google.cloud import bigquery
    client = bigquery.Client()
    bucket_name = 'venti-dev-498572-bqoutput'
    project = "venti-dev-498572"
    dataset_id = "final_output"
    table_id = "final_financial_and_risk"

    destination_uri = "gs://{}/{}".format(bucket_name, "output_risk.csv")
    dataset_ref = client.dataset(dataset_id, project=project)
    table_ref = dataset_ref.table(table_id)

    extract_job = client.extract_table(
        table_ref,
        destination_uri,
        # Location must match that of the source table.
        location="US",
    )  # API request
    extract_job.result()  # Waits for job to complete.

    print(
        "Exported {}:{}.{} to {}".format(project, dataset_id, table_id, destination_uri)
    )

def exportToCSV_transaction():
    from google.cloud import bigquery
    client = bigquery.Client()
    bucket_name = 'venti-dev-498572-bqoutput'
    project = "venti-dev-498572"
    dataset_id = "final_output"
    table_id = "final_transaction_round2"

    #wildcard is used so that the file can be sharded into a number of extra files
    destination_uri = "gs://{}/{}".format(bucket_name, "output_transactions-*.csv")
    dataset_ref = client.dataset(dataset_id, project=project)
    table_ref = dataset_ref.table(table_id)

    extract_job = client.extract_table(
        table_ref,
        destination_uri,
        # Location must match that of the source table.
        location="US",
    )  # API request
    extract_job.result()  # Waits for job to complete.
    print(
        "Exported {}:{}.{} to {}".format(project, dataset_id, table_id, destination_uri)
    )
    
def archiveData():
    import base64
    from datetime import datetime
    client = storage.Client()
    bucket = client.bucket('venti-dev-498572-bqoutput')
    blobs = list(bucket.list_blobs(prefix=''))
    destination_bucket = client.get_bucket('venti-dev-498572-archive')
    current_time = datetime.now()
    dt_string = current_time.strftime("%Y-%m-%d_%H%M")
    for i in blobs:
        print(str(blob2dict(i)['Name']) + '\n')
        if blob2dict(i)['Name'].startswith('output_'):
            new_blob_name = ('' + dt_string + '/' + blob2dict(i)['Name'])
            new_blob = bucket.copy_blob(
                i, destination_bucket, new_blob_name)
            print('Archived: ' + str(blob2dict(i)['Name']))
                
def blob2dict(blob):
    """Converts a google.cloud.storage.Blob (which represents a storage object) to context format (GCS.BucketObject)."""
    return {
        'Name': blob.name,
        'Bucket': blob.bucket.name,
        'ContentType': blob.content_type,
        'TimeCreated': blob.time_created,
        'TimeUpdated': blob.updated,
        'TimeDeleted': blob.time_deleted,
        'Size': blob.size,
        'MD5': blob.md5_hash,
        'OwnerID': '' if not blob.owner else blob.owner.get('entityId', ''),
        'CRC32c': blob.crc32c,
        'EncryptionAlgorithm': blob._properties.get('customerEncryption', {}).get('encryptionAlgorithm', ''),
        'EncryptionKeySHA256': blob._properties.get('customerEncryption', {}).get('keySha256', ''),
    } 
	

#----------------------------------#
# Function that starts a VM
#----------------------------------#

const Compute = require('@google-cloud/compute');
const compute = new Compute();

/**
 * Starts Compute Engine instances.
 *
 * Expects a PubSub message with JSON-formatted event data containing the
 * following attributes:
 *  zone - the GCP zone the instances are located in.
 *  label - the label of instances to start.
 *
 * @param {!object} event Cloud Function PubSub message event.
 * @param {!object} callback Cloud Function PubSub callback indicating
 *  completion.
 */
exports.startInstancePubSub = async (event, context, callback) => {
  try {
    const payload = _validatePayload(
      JSON.parse(Buffer.from(event.data, 'base64').toString())
    );
    const options = {filter: `labels.${payload.label}`};
    const [vms] = await compute.getVMs(options);
    await Promise.all(
      vms.map(async instance => {
        if (payload.zone === instance.zone.id) {
          const [operation] = await compute
            .zone(payload.zone)
            .vm(instance.name)
            .start();

          // Operation pending
          return operation.promise();
        }
      })
    );

    // Operation complete. Instance successfully started.
    const message = `Successfully started instance(s)`;
    console.log(message);
    callback(null, message);
  } catch (err) {
    console.log(err);
    callback(err);
  }
};

/**
 * Validates that a request payload contains the expected fields.
 *
 * @param {!object} payload the request payload to validate.
 * @return {!object} the payload object.
 */
const _validatePayload = payload => {
  if (!payload.zone) {
    throw new Error(`Attribute 'zone' missing from payload`);
  } else if (!payload.label) {
    throw new Error(`Attribute 'label' missing from payload`);
  }
  return payload;
};


#----------------#
# package.json
#----------------#

{
  "name": "cloud-functions-schedule-instance",
  "version": "0.1.0",
  "private": true,
  "license": "Apache-2.0",
  "author": "Google Inc.",
  "repository": {
    "type": "git",
    "url": "https://github.com/GoogleCloudPlatform/nodejs-docs-samples.git"
  },
  "engines": {
    "node": ">=8.0.0"
  },
  "scripts": {
    "test": "mocha test/*.test.js --timeout=20000"
  },
  "devDependencies": {
    "@google-cloud/nodejs-repo-tools": "^3.3.0",
    "mocha": "^7.0.0",
    "proxyquire": "^2.0.0",
    "sinon": "^8.0.0"
  },
  "dependencies": {
    "@google-cloud/compute": "^1.0.0"
  }
}




