BigQuery

Querying Data & Schema Design

How a Query Works
1) Data is stored in distributed storage and called by 'workers' called slots
2) Stage 1: The slots look for matches based on the WHERE clause and then return results to 1 big slots
3) Stage 2: Then BQ gets the results from the 1 slot (this stage will have the longest wait time since the other smaller slots have to find matches first)
You can see how many slots by looking at the Output of the Stage in the query execution plan - in GCP BQ, it's under the view 'Execution Details' tab.
Slots are 'fairly divided' amongst jobs by BQ. If you have a high priority job, you can reserve x number of slots for it - hierarchical reservation.
Guide: 2000 BQ slots for 50 medium complexity queries simultaneously.
If the reservation is too small, the query will still run until done just one after another BUT BE WARNED queries get timed out after 6 hours - symptom, see if the query gets scheduled in less than a second and if it's more, then there's a problem.

Reference: Query Plan
https://cloud.google.com/bigquery/query-plan-explanation
Stage timings are percentages of the whole job
WAIT, READ, COMPUTE, WRITE are the most common steps in a stage
You can get the query plan via API and it'll return a JSON - call jobs.get

ANSI SQL queries - ACID compliant
Failed queries are free - e.g. syntax error

Partitioning & Clustering
https://cloud.google.com/bigquery/docs/partitioned-tables

Can partition - on a datetype or int type (ingestion time, date/timestamp, integer range)
Partitioning can be based on: 1) ingestion time, 2) a TIMESTAMP or DATE column, 3) a INT column (specify min 0, max 100, buckets/intervals)- if you want to partition on a STR column, could transform the string and mod to 1000 before treating it as a INT column. 
When quering a table partitioned by ingestion time, specify WHERE _PARTITIONTIME > TIMESTAMP_SUB(TIMESTAMP('yyyy-mm-dd'), INTERVAL 5 DAY). Creation syntax --time_partitioning_day   --partition by 'etcetc'.
When partitioning by TIMESTAMP or DATE or INT, 2 special additional partitions are also created - __NULL__ & __UNPARTITIONED__ for nulls and values outside the allowed range of dates/ints (before 0001-01-01 or after 9999-12-31 - note that DML statements cannot ref prior 1970-01-01 or after 2159-12-31)
As an alternative to partitioning, you could shard the tables but it's not as good as partitioning because each of the sharded tables need to have a copy of the schema and metadata, and permissions need to be verified for every single table. Also, you can query up to 4000 partitions but only up to 1000 tables.

Can cluster - but must be partitioned first
If you know that you'll always want to query your table based on one of the columns (if >50% of queries use a particular where clause), you should cluster the table based on that column - e.g. if you always want to query based on a particular user, should cluster on userID. Can also cluster based on multiple columns just by doing column1, column2 - like indexing. MUST already be partitioned by DATE or TIMESTAMP column to perform clustering.
You should opt for clustering if: you don't care how the data will be clustered and let BQ automatically figure out how the data should be clustered for optimal performance and cost AND/OR you need more than 4000 partitions (e.g. probable use case - IDs of some sort)
You can partition and cluster on the same column to get the benefits of both. First partition according to the specified range, within each range data is clustered. You can see the upper bound of query cost before you run a query - could be less if the cluster is much smaller than the partition.
Note: partitioning and clustering have to be done at table creation - create a new table and copy the data if you've already created the table
For tables that take streaming/live data, the latest day goes into a new partition but that new data is not clustered - the older partitions are clustered and are sorted. BigQuery then adds the new partition data to the other clusters on a schedule. If you want to force a recluster, use DML in the necessary partition. Reclustering is a system event. You can set require_partitioning_filter so that any queries on the table must include a partition clause


Wildcard table query
https://cloud.google.com/bigquery/docs/querying-wildcard-tables
https://cloud.google.com/bigquery/docs/reference/standard-sql/wildcard-table-reference
Standard SQL - use backticks, asterisk, and the _TABLE_SUFFIX function (`gsod-public.gsod_dataset.gsod_table*` WHERE _TABLE_SUFFIX BETWEEN '20190101' AND '20191231')
Legacy SQL - FROM TABLE_DATE_RANGE([mydataset.sea_weather_], TIMESTAMP("2016-05-01"), TIMESTAMP("2016-05-09"))
Does not work on views
Does not work on external/federated data sets - i.e. only works on BQ native storage
Does not utilise caching so every single wildcard query gets billed
Cannot be used as part of DML UPDATE statement target table - but can be used as a FROM table

For big tables (>10GB) create nested and repeated fields instead of normalising - unless you plan to perform UPDATE or DELETE on it often - FROM table, UNNEST(columnname)
Use temp tables instead of WITH clauses where possible since it doesn't improve cost or performance

APPROX_COUNT_DISTINCT() has 1% error margin but is much faster than COUNT_DISTINCT()
Use LIMIT if you want to ORDER BY a whole table
If pipeline needs filters, do all the filters first so you work with less data downstream
If pipeline involves joining, join the largest and smallest tables first so that you work with less data downstream
Use _TABLES_SUMMARY_ instead of _TABLES_ because it's cheaper.
SELECT count(*) is free, SELECT count(columname) is not free
ETL or ELT - If ETL reduces the amount to upload, then do it. If not, ELT. BQ service for this is BigQuery Transfer Service. An agent manages recurring data transfers.
When updating data, if the data is normalised, just add newer updated values to the reference/fact table and reference the later start/creation date. If the data is denormalised, perform updates on a separate view/partition and then switch it out with the live/production one. If you want to change something based on a particular customer name, for example, put the updated values in a separate table and then use an outer join to append the updates.
NOTE: Scripting (stored procedures) aren't a feature with BQ yet. Gotta write stuff in Python until it's out of Alpha and released to public.
Donâ€™t use BQ for outlier time series analysis - BigTable is better for that

Reference for legit DML queries
https://cloud.google.com/bigquery/docs/reference/standard-sql/data-manipulation-language

Aliasing
https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax#explicit-alias-visibility
Only clauses after the alias has been made can reference said alias

Do's & Don'ts
https://cloud.google.com/bigquery/docs/best-practices-performance-overview

Input data and data sources (I/O): How many bytes does your query read?
Communication between nodes (shuffling): How many bytes does your query pass to the next stage? How many bytes does your query pass to each slot?
Computation: How much CPU work does your query require?
Outputs (materialization): How many bytes does your query write?
Query anti-patterns: Are your queries following SQL best practices?

https://cloud.google.com/bigquery/docs/best-practices-performance-patterns

Don't - self-join
Do - window function
analytic_function_name ( [ argument_list ] )
  OVER (
    [ PARTITION BY partition_expression_list ]
    [ ORDER BY expression [{ ASC | DESC }] [, ...] ]
    [ window_frame_clause ]
  )
  
Don't - have data skew - symptom max compute time is much higher than average compute time in query plan
Do - Filter your data as early as possible & use an approximate aggregate function such as APPROX_TOP_COUNT to determine if the data is skewed - https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators#approx_top_count
SELECT APPROX_TOP_COUNT(x, 2) as approx_top_count
FROM UNNEST(["apple", "apple", "pear", "pear", "pear", "banana"]) as x;
+-------------------------+
| approx_top_count        |
+-------------------------+
| [{pear, 3}, {apple, 2}] |
+-------------------------+

Don't - CROSS JOIN on all records
Do - if you must CROSS JOIN, GROUP BY to pre-aggregate the data or use a window function instead

Don't - use point-specific DML (single row updates) because BQ works on OLAP (Online Analytical Processing) not OLTP (Online Transaction Processing) - if you need OLTP, opt for Cloud SQL instead
Do - UPDATE & DELETE in batches. avoid INSERT single rows


BQ Functions
https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators

SAFE.function_name()
returns NULL instead of an error
e.g. for SUBSTR function that normally does not support negative values

SAFE_CAST("apple" AS INT64)

SAFE_DIVIDE(X, Y)
SAFE_MULTIPLY(X, Y)
SAFE_NEGATE(X)
SAFE_ADD(X, Y)
SAFE_SUBTRACT(X, Y)
returns NULL instead of errors



-----------------------------------------------------------------


Loading into BQ native storage
https://cloud.google.com/bigquery/docs/loading-data
Only GCS and local machine
Allows you to use the whole BQ query capability - wildcard tables, partitioning, clustering, query caching

Both batch & stream - batch load is free, streaming is not. But because loading data is free, there is a max 1000 per day quota. Failed load jobs count towards this quota. Max 15TB combined. Max 10M individual files.
2 types of storage: active & long-term. Best practice is to put expiration dates on your data partitions upon creation/initiation or as a default config. Note: important that you expire partitions and not delete data because expiring partitions is free since it's considered as a different type of event than deletion
If the schema changes sometimes, you can set Schema "Automatically detect" - https://cloud.google.com/bigquery/docs/schema-detect
Automatically shards & shuffles data but that isnâ€™t visible to users since itâ€™s serverless
Batchload formats: 1st) best for BQ are parquet (apache) or avro (hadoop) - internally compressed, self describing, comes with original schema; 2nd) json - new line means new row - do not indent or make it pretty - supports nesting, no schema; 3rd) csv, no schema, no nesting. Note: orc is a newer format that's like parquet and avro
Regional Cloud Storage is cheaper than BQ storage
If load CSV but it was not flagged as CSV, then it would error immediately.
If load CSV but some rows were skipped on import, it would say that data is not fully imported.

Reference: Loading avro files from GCS - how-to in console & CLI, IAM/permission requirements
https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro

Reference: Loading data using BQ Data Transfer Service
https://cloud.google.com/bigquery-transfer/docs/transfer-service-overview
Campaign Manager
Cloud Storage (beta)
Google Ad Manager
Google Ads
Google Merchant Center (beta)
Google Play (beta)
Search Ads 360 (beta)
YouTube Channel reports
YouTube Content Owner reports
Amazon S3 (beta)
Teradata (beta)
Amazon Redshift (beta)


-----------------------------------------------------------------


Federated Data Sources
https://cloud.google.com/bigquery/external-data-sources
https://cloud.google.com/bigquery/external-data-drive
With Bigtable, GCS, Google Drive
Good for: loading & cleaning in one pass into BQ, small amount of frequently changing data that you join with other tables
Fastest query times: BQ native storage, GCS, Google Drive
No wildcard table queries
No partitioning or clustering
In BQ native, queries get cached so that if you run the same query multiple times, it doesnâ€™t cost extra. If using an external table, then you lose this benefit.
If using GCS or Bigtable, they need to be in the same geographic location. No location limits if pulling from Google Drive.


Stackdriver Logging
https://cloud.google.com/logging/docs/export/configure_export_v2


-----------------------------------------------------------------


BigQuery ML

Available models:
Linear regression
Binary logistic regression
Multiclass logistic regression â€“ max 50 categories â€“ uses multinomial classifier + cross entropy loss function
K-means clustering
Tensorflow model import â€“ takes previously trained Tensorflow models and get predictions using BQ ML

CREATE MODEL statement â€“ quota limit = 1000 queries per day per project
https://cloud.google.com/bigquery-ml/quotas

2 types of pricing: Flat-rate & On-demand
https://cloud.google.com/bigquery-ml/pricing
* You can only change your pricing model on a monthly basis
* Model creation â€“ 50 iterations max â€“ first 10GB of data processed by CREATE MODEL per month is free under the BQ free tier
* Evaluation, inspection, prediction â€“ first 1 TB of data per month under BQ free tier
Best way to estimate costs is to use Stackdriver audit logs to see the bytes billed by BQ ML


-----------------------------------------------------------------

BQ CLI

Reference: How to bq commands
https://cloud.google.com/bigquery/docs/bq-command-line-tool
https://cloud.google.com/bigquery/docs/reference/bq-cli-reference






